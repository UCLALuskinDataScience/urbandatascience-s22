{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "545b3a7a",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "## Lecture objectives\n",
    "1. Understand the purposes and potential uses of clustering\n",
    "2. Learn more exploratory data analysis techniques, such as pairplots\n",
    "3. Learn how to implement k-means clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e226ed",
   "metadata": {},
   "source": [
    "## Why cluster?\n",
    "Cluster analysis is an exploratory technique to identify sensible groupings in a dataset. The analyst has no prior knowledge of what these clusters are, and the data are not labeled with the \"correct\" cluster. Thus, cluster analysis is an *unsupervised* machine learning technique.\n",
    "\n",
    "Some potential applications of clustering:\n",
    "* Identify types of Marine Protected Area (e.g., [Bohorquez et al. 2019](https://www.sciencedirect.com/science/article/pii/S0308597X19304439))\n",
    "* Identify types of street networks (e.g., [Barrington-Leigh and Millard-Ball 2020](https://www.pnas.org/content/117/4/1941))\n",
    "* Identify types of neighborhood (e.g., [Kendig 2007](https://www.tandfonline.com/doi/abs/10.1080/01944367608977731))\n",
    "* Identify types of transit agencies (e.g. [Ederer et al. 2019](https://journals.sagepub.com/doi/full/10.1177/0361198119852074))\n",
    "* Identify patterns of cruising for parking (e.g. [Millard-Ball, Weinberger & Hampshire 2021](https://findingspress.org/article/28061-the-shape-of-cruising))\n",
    "\n",
    "Clustering the data in this way can help you see regularities in the data that you can then interpret. It might suggest policies that are appropriate for one group of cities or agencies but not another. Or it could identify a peer group against which to benchmark (say) affordable housing construction costs or transit on-time performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314e7b24",
   "metadata": {},
   "source": [
    "## Types of clustering\n",
    "Formally, clustering takes a set of *N* objects and finds *K* groups based on a measure of similarity. For a technical yet accessible overview, I recommend [Jain 2010](https://www.sciencedirect.com/science/article/abs/pii/S0167865509002323). \n",
    "\n",
    "The two broad groups of clustering algorithms are *hierarchical* and *partitional*. \n",
    "\n",
    "Ederer et al. (2019), for example, use a hierarchical algorithm to classify transit agencies.\n",
    "\n",
    "<img src=\"https://journals.sagepub.com/na101/home/literatum/publisher/sage/journals/content/trra/2019/trra_2673_11/0361198119852074/20191218/images/medium/10.1177_0361198119852074-fig1.gif\" style=\"width:50%;\"/>\n",
    "\n",
    "\n",
    "But let's start with a partitional algorithm. The most popular is called *k-means*. Again, this is an exploratory data analysis process. The analyst needs to specify the number of clusters *K*, and should experiment with different values of *K* until a meaningful grouping emerges. Another way to choose *K* is the [elbow method](https://en.wikipedia.org/wiki/Elbow_method_(clustering)), but we won't discuss that here.\n",
    "\n",
    "## Example: precinct-level voting\n",
    "We'll use the `sklearn` library to implement the k-means algorithm. The aim: identify a typology of voters based on precinct-level data.\n",
    "\n",
    "The California [Statewide Database](https://statewidedatabase.org), maintained by UC Berkeley, provides access to voting data. Your GitHub repository should include the November 2020 precinct data for Los Angeles County."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a43ed3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/c037_g20_sov_data_by_g20_srprec.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b14775",
   "metadata": {},
   "source": [
    "The unique identifier is given by the precinct column, `srprec`, so let's set that as the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04a6277",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('srprec', inplace=True)\n",
    "df.index.is_unique  # verify that it's unique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd528ac9",
   "metadata": {},
   "source": [
    "What columns are in the data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e00498f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d700fa9",
   "metadata": {},
   "source": [
    "There are obviously a lot of columns. You can see the [full codebook here](https://statewidedatabase.org/d10/g20.html). But the propositions are pretty self explanatory. `PRSDEM01` gives votes for Biden, `PRSREP01` for Trump, etc. Note the state Senate and Assembly races will be different depending on the precinct, so let's ignore those.\n",
    "\n",
    "What do the data in these columns look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa91f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['PRSDEM01','PRSREP01']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac605a95",
   "metadata": {},
   "source": [
    "We see that the numbers are in absolute terms. Let's convert them to vote share."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ca238a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is share of two-party vote (ignoring other candidates)\n",
    "df['Biden_pc'] = df.PRSDEM01 / (df.PRSDEM01+df.PRSREP01)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0a5ee5",
   "metadata": {},
   "source": [
    "Let's do the same for each proposition too. \n",
    "\n",
    "How can we get a list with the numbers of the propositions? We'll use a list comprehension to get the list of relevant columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8d9573",
   "metadata": {},
   "outputs": [],
   "source": [
    "props = [col for col in df.columns if col.startswith('PR_') and col.endswith('Y')]\n",
    "print(props)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3450ce7b",
   "metadata": {},
   "source": [
    "And another list comprehension to get the relevant digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c123062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use our string indexing to get the 4th and 5th characters\n",
    "# for example\n",
    "print('PR_14_Y'[3:5])\n",
    "\n",
    "# apply this in a list comprehension\n",
    "props = [prop[3:5] for prop in props]\n",
    "print(props)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5234ca6a",
   "metadata": {},
   "source": [
    "Now, let's loop over this list of propositions to calculate the vote share."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367f1c62",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "for prop in props:\n",
    "    df[prop+'_pc_yes'] = df['PR_'+prop+'_Y'] / (df['PR_'+prop+'_Y'] \n",
    "                                              + df['PR_'+prop+'_N'])*100\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d768265",
   "metadata": {},
   "source": [
    "### Inspect and standardize the data\n",
    "\n",
    "Number 1 rule: before applying any algorithm to your data, look at it!\n",
    "\n",
    "We could create a scatterplot matrix manually. But `seaborn` has a [nice function to do this for us](https://seaborn.pydata.org/examples/scatterplot_matrix.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8b4ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.pairplot?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1e6797",
   "metadata": {},
   "source": [
    "We'll plot a subset of the columns, ignoring a few of the less critical propositions. Should stem cell research and dialysis rules really be on the ballot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d0b950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of all the columns with 'pc' in the name\n",
    "cols_to_plot = [col for col in df.columns if '_pc' in col]\n",
    "\n",
    "# remove those we don't want\n",
    "cols_to_plot.remove('14_pc_yes') \n",
    "cols_to_plot.remove('23_pc_yes') \n",
    "cols_to_plot.remove('24_pc_yes') \n",
    "\n",
    "# kind='reg' adds the line of best fit\n",
    "ax = sns.pairplot(df[cols_to_plot], kind='reg')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460f0632",
   "metadata": {},
   "source": [
    "There are pretty strong relationships between Presidential voting and the propositions. All have a positive correlation except for Prop 20 (harsher sentencing) and Prop 22 (independent contractor status for drivers for Uber, Doordash, etc.). [A helpful reminder of the propositions is here](https://ballotpedia.org/California_2020_ballot_propositions).\n",
    "\n",
    "But there isn't a perfect relationship. Perhaps cluster analysis can reveal some groupings?\n",
    "\n",
    "First, it helps to pre-process the data in two ways:\n",
    "* Let's align the data so that a higher percent means more progressive. This means using the percent \"no\" for Props 20 and 22\n",
    "* We should standardize each variable to mean zero and standard deviation one. This helps ensure that the distances in multidimensional space are consistent. (Since we have a percentage measure, it won't make much difference compared to a variable like population, but it's good practice.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67de41d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prop in ['20','22']:\n",
    "    df[prop+'_pc_no'] = 100 - df[prop+'_pc_yes']\n",
    "    df.drop(columns=[prop+'_pc_yes'], inplace=True)\n",
    "\n",
    "# then rerun the same code as above\n",
    "cols_to_plot = [col for col in df.columns if '_pc' in col]\n",
    "cols_to_plot.remove('14_pc_yes') \n",
    "cols_to_plot.remove('23_pc_yes') \n",
    "cols_to_plot.remove('24_pc_yes') \n",
    "\n",
    "# see https://scikit-learn.org/stable/modules/preprocessing.html for standardization\n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler().fit(df[cols_to_plot])\n",
    "\n",
    "# as in the previous lecture, \n",
    "# the scaler returns a numpy array, so we cast this as a DataFrame \n",
    "# and need to specify the column names and index\n",
    "df_scaled = pd.DataFrame(scaler.transform(df[cols_to_plot]), \n",
    "                         columns=cols_to_plot, index=df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ed323c",
   "metadata": {},
   "source": [
    "Let's check that our data still look reasonable by rerunning the same pairplot.\n",
    "\n",
    "Notice that the y axes run from about -3 to +3. This should be true for any standardized variable, as most observations are within 3 standard deviations of the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6ef07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.pairplot(df_scaled, kind='reg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0220a9d0",
   "metadata": {},
   "source": [
    "### KMeans in scikit-learn\n",
    "The documention has some useful examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77695b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "KMeans?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84adcf61",
   "metadata": {},
   "source": [
    "We can specify the number of clusters. Optionally, we can specify the random state, so that we can reproduce our work. \n",
    "\n",
    "Then we fit to our data, in this case `df_scaled`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5079d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=5, random_state=0).fit(df_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2387faa4",
   "metadata": {},
   "source": [
    "Here, the error message is pretty helpful. Let's drop the NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0040a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_scaled))\n",
    "df_scaled = df_scaled.dropna()\n",
    "print(len(df_scaled))\n",
    "\n",
    "kmeans = KMeans(n_clusters=5, random_state=1).fit(df_scaled)\n",
    "print(kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf4127a",
   "metadata": {},
   "source": [
    "It's not immediately obvious what we can do with this `KMeans` object. But let's explore it. Use the tab autocomplete to see the different methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a045717b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kmeans."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d98a82f",
   "metadata": {},
   "source": [
    "The cluster centers are defined by the (standardized) value for each variable. Each observation is assigned to the cluster with the closest center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7d98b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d698805d",
   "metadata": {},
   "source": [
    "Notice that the `cluster_centers_` is an array that is `K x L`, where `K` is the number of clusters and `L` is the number of variables.\n",
    "\n",
    "Here, we have 5 clusters, and we used 10 variables to define the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0ee454",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kmeans.cluster_centers_.shape)\n",
    "print(len(df_scaled.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c5ac34",
   "metadata": {},
   "source": [
    "The `labels_` attribute gives the label of the cluster to which each observation (i.e., each precinct) is assigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a01c532",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2c3238",
   "metadata": {},
   "source": [
    "So there are as many labels as rows in our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9a0f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kmeans.labels_.shape)\n",
    "print(len(df_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1be6d04",
   "metadata": {},
   "source": [
    "That means that we can simply add the cluster id back to our original dataframe!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afee2c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled['cluster_id'] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2989c7a6",
   "metadata": {},
   "source": [
    "How large is each cluster? Note that the algorithm doesn't aim to produce equal-size groupings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed68467",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled.groupby('cluster_id').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebc3230",
   "metadata": {},
   "source": [
    "### Visualizing the clusters\n",
    "How best to visualize what the clusters mean? If we had just two columns, a scatterplot with a color code for each cluster would work well. But we have 10 dimensions.\n",
    "\n",
    "One way is to redo our original scatter plot matrix, but with each cluster indicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b828f25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.pairplot(df_scaled, hue='cluster_id', )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63542349",
   "metadata": {},
   "source": [
    "My preferred option, however, is a radar chart. Neither `seaborn` nor `matplotlib` do this natively, but [there is an example in the `matplotlib` gallery](https://matplotlib.org/stable/gallery/specialty_plots/radar_chart.html). I've just copied and pasted that code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cea29bc",
   "metadata": {
    "code_folding": [
     13
    ]
   },
   "outputs": [],
   "source": [
    "# code from https://matplotlib.org/stable/gallery/specialty_plots/radar_chart.html\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Circle, RegularPolygon\n",
    "from matplotlib.path import Path\n",
    "from matplotlib.projections.polar import PolarAxes\n",
    "from matplotlib.projections import register_projection\n",
    "from matplotlib.spines import Spine\n",
    "from matplotlib.transforms import Affine2D\n",
    "\n",
    "\n",
    "def radar_factory(num_vars, frame='circle'):\n",
    "    \"\"\"\n",
    "    Create a radar chart with `num_vars` axes.\n",
    "\n",
    "    This function creates a RadarAxes projection and registers it.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_vars : int\n",
    "        Number of variables for radar chart.\n",
    "    frame : {'circle', 'polygon'}\n",
    "        Shape of frame surrounding axes.\n",
    "\n",
    "    \"\"\"\n",
    "    # calculate evenly-spaced axis angles\n",
    "    theta = np.linspace(0, 2*np.pi, num_vars, endpoint=False)\n",
    "\n",
    "    class RadarAxes(PolarAxes):\n",
    "\n",
    "        name = 'radar'\n",
    "        # use 1 line segment to connect specified points\n",
    "        RESOLUTION = 1\n",
    "\n",
    "        def __init__(self, *args, **kwargs):\n",
    "            super().__init__(*args, **kwargs)\n",
    "            # rotate plot such that the first axis is at the top\n",
    "            self.set_theta_zero_location('N')\n",
    "\n",
    "        def fill(self, *args, closed=True, **kwargs):\n",
    "            \"\"\"Override fill so that line is closed by default\"\"\"\n",
    "            return super().fill(closed=closed, *args, **kwargs)\n",
    "\n",
    "        def plot(self, *args, **kwargs):\n",
    "            \"\"\"Override plot so that line is closed by default\"\"\"\n",
    "            lines = super().plot(*args, **kwargs)\n",
    "            for line in lines:\n",
    "                self._close_line(line)\n",
    "\n",
    "        def _close_line(self, line):\n",
    "            x, y = line.get_data()\n",
    "            # FIXME: markers at x[0], y[0] get doubled-up\n",
    "            if x[0] != x[-1]:\n",
    "                x = np.append(x, x[0])\n",
    "                y = np.append(y, y[0])\n",
    "                line.set_data(x, y)\n",
    "\n",
    "        def set_varlabels(self, labels):\n",
    "            self.set_thetagrids(np.degrees(theta), labels)\n",
    "\n",
    "        def _gen_axes_patch(self):\n",
    "            # The Axes patch must be centered at (0.5, 0.5) and of radius 0.5\n",
    "            # in axes coordinates.\n",
    "            if frame == 'circle':\n",
    "                return Circle((0.5, 0.5), 0.5)\n",
    "            elif frame == 'polygon':\n",
    "                return RegularPolygon((0.5, 0.5), num_vars,\n",
    "                                      radius=.5, edgecolor=\"k\")\n",
    "            else:\n",
    "                raise ValueError(\"Unknown value for 'frame': %s\" % frame)\n",
    "\n",
    "        def _gen_axes_spines(self):\n",
    "            if frame == 'circle':\n",
    "                return super()._gen_axes_spines()\n",
    "            elif frame == 'polygon':\n",
    "                # spine_type must be 'left'/'right'/'top'/'bottom'/'circle'.\n",
    "                spine = Spine(axes=self,\n",
    "                              spine_type='circle',\n",
    "                              path=Path.unit_regular_polygon(num_vars))\n",
    "                # unit_regular_polygon gives a polygon of radius 1 centered at\n",
    "                # (0, 0) but we want a polygon of radius 0.5 centered at (0.5,\n",
    "                # 0.5) in axes coordinates.\n",
    "                spine.set_transform(Affine2D().scale(.5).translate(.5, .5)\n",
    "                                    + self.transAxes)\n",
    "                return {'polar': spine}\n",
    "            else:\n",
    "                raise ValueError(\"Unknown value for 'frame': %s\" % frame)\n",
    "\n",
    "    register_projection(RadarAxes)\n",
    "    return theta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9d98e9",
   "metadata": {},
   "source": [
    "I adapted this example, putting it in a function called `radar_plot` that takes two arguments:\n",
    "* the `kmeans` object\n",
    "* the dataframe with the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2335b403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def radar_plot(kmeans, df_scaled):\n",
    "    N  = kmeans.cluster_centers_.shape[1]  # number of columns / variables\n",
    "    k = kmeans.n_clusters\n",
    "    theta = radar_factory(N, frame='polygon')\n",
    "    data = kmeans.cluster_centers_.T\n",
    "    spoke_labels = [col for col in df_scaled.columns if col!='cluster_id']\n",
    "    fig, ax = plt.subplots(figsize=(9, 9),\n",
    "                                subplot_kw=dict(projection='radar'))\n",
    "    fig.subplots_adjust(wspace=0.25, hspace=0.20, top=0.85, bottom=0.05)\n",
    "\n",
    "    ax.plot(theta, data) #, color=color)\n",
    "    ax.set_varlabels(spoke_labels)\n",
    "\n",
    "    # add legend relative to top-left plot\n",
    "    labels = ['Cluster {}'.format(kk) for kk in range(k)]\n",
    "    ax.legend(labels, loc=(0.9, .95),\n",
    "                                labelspacing=0.1, fontsize='small')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ce53fc",
   "metadata": {},
   "source": [
    "Let's call this function with our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3f1161",
   "metadata": {},
   "outputs": [],
   "source": [
    "radar_plot(kmeans, df_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf59e99",
   "metadata": {},
   "source": [
    "### Exploring different numbers of clusters\n",
    "Here, the interesting finding is that all the clusters form concentric circles. There isn't a cluster of precincts that (say) votes against rent control but is progressive on the other items on the ballot.\n",
    "\n",
    "We can certainly find these clusters if we increase `k`, but then these \"weird\" clusters have few precincts.\n",
    "\n",
    "For example, let's try with `k=10`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8840f2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the old cluster id, so that we don't include it in our new estimates\n",
    "df_scaled.drop(columns=['cluster_id'], inplace=True)  \n",
    "\n",
    "# this is the same code as before\n",
    "kmeans = KMeans(n_clusters=10, random_state=1).fit(df_scaled)\n",
    "df_scaled['cluster_id'] = kmeans.labels_\n",
    "print(df_scaled.groupby('cluster_id').size())\n",
    "radar_plot(kmeans, df_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c048d1",
   "metadata": {},
   "source": [
    "Let's go back to our original 5 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617d3db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled.drop(columns=['cluster_id'], inplace=True) \n",
    "kmeans = KMeans(n_clusters=5, random_state=1).fit(df_scaled)\n",
    "df_scaled['cluster_id'] = kmeans.labels_\n",
    "radar_plot(kmeans, df_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88b76fc",
   "metadata": {},
   "source": [
    "### Mapping the clusters\n",
    "The Statewide Database team provide geographic boundary files as well as the vote counts. The shapefile for Los Angeles count is in your GitHub respository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530bbbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "gdf = gpd.read_file('data/srprec_037_g20_v01_shp/srprec_037_g20_v01.shp')\n",
    "gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb926639",
   "metadata": {},
   "source": [
    "Note that there is no projection file, so geopandas doesn't know the coordinate system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a441135",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gdf.crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2227f92",
   "metadata": {},
   "source": [
    "The documentation online says it's in lat/lon, so let's set it to EPSG 4326."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d4505f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.crs = 'EPSG:4326'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfa165e",
   "metadata": {},
   "source": [
    "Before we do a join, let's look at the data to figure out the number of rows and the join column, and whether `srprec` is a unique identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a6d1fc",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# looks like we can join on srprec, \n",
    "# but we'll need to set that as the index for gdf\n",
    "print(df_scaled.head())\n",
    "print(gdf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c077aaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have more observations in our spatial data, so we can do a left join to that\n",
    "# maybe some precincts have no voters?\n",
    "print(len(gdf))\n",
    "print(len(df_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04733a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# both are unique, which makes things easier\n",
    "print(df_scaled.index.is_unique)\n",
    "print(gdf.SRPREC.is_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41220f2",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# do the join\n",
    "gdf.set_index('SRPREC', inplace=True)\n",
    "joinedGdf = gdf.join(df_scaled)\n",
    "joinedGdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa373e5",
   "metadata": {},
   "source": [
    "Let's map the clusters. We should color code by `cluster_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9a8353",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import contextily as ctx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "joinedGdf.to_crs('EPSG:3857').plot('cluster_id', legend=True, ax = ax, alpha=0.4)\n",
    "ctx.add_basemap(ax, zoom=12, source=ctx.providers.Stamen.TonerLite)\n",
    "\n",
    "# drop Catalina Island\n",
    "ax.set_ylim([3.98e6, 4.14e6])\n",
    "\n",
    "# and we really don't need the axis ticks and labels, so we set them to an empty list\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "\n",
    "ax.set_title('Typology of voting, 2020 General Election', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2848cd",
   "metadata": {},
   "source": [
    "What can we do to improve the map?\n",
    "\n",
    "The `source` keyword gives access to lots of options. Take a look at the possibilities with `ctx.providers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2637da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx.providers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a84482",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Exercise:</strong> How else would you improve the map?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482adcf2",
   "metadata": {},
   "source": [
    "There's no right answer here, but I first replace the missing data with an explicit \"no data\" label. To do that, we need to change the data type of `cluster_id` to string.\n",
    "\n",
    "We can also remove the decimal point from the other cluster labels using the `str.replace()` function. We replace `.0` with an empty string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e5acf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "joinedGdf.cluster_id = joinedGdf.cluster_id.astype(str)\n",
    "joinedGdf.cluster_id = joinedGdf.cluster_id.str.replace('.0', '')\n",
    "joinedGdf.cluster_id = joinedGdf.cluster_id.str.replace('nan', 'No data')\n",
    "\n",
    "joinedGdf.cluster_id.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c98ba1",
   "metadata": {},
   "source": [
    "In the plot itself, we might:\n",
    "* replace the colorbar with a legend. This is because we have discrete categories (0-5), not a continuous variable. That is done with the `categorical=True` keyword argument.\n",
    "* add a legend title. We get the legend and then use the `set_title()` function.\n",
    "* specify the colors. I find https://colorbrewer2.org the most helpful. \n",
    "* specify a gray for missing data (a grayscale color is a string between 0 and 1. E.g. 0 is black and 1 is white, with values in between representing progressively lighter shades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a34df4",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# getting the colors into a colormap required some searching\n",
    "# https://stackoverflow.com/questions/38882233/geopandas-matplotlib-plot-custom-colors\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "cmap = LinearSegmentedColormap.from_list(\n",
    "    'mycmap', [(0, '#7fc97f'), (0.2, '#beaed4'), (0.4, '#fdc086'), \n",
    "               (0.6, '#ffff99'), (0.8, '#386cb0'), (1.0, '0.5')])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "# in the video, I forgot to add the cmap=cmap argument, so the colormap above was being ignored\n",
    "joinedGdf.to_crs('EPSG:3857').plot('cluster_id', ax=ax, categorical=True, \n",
    "                                  legend=True, alpha=0.4, cmap=cmap,\n",
    "                                  legend_kwds={'loc': 'upper left'})\n",
    "\n",
    "# add a legend title\n",
    "legend = ax.get_legend()\n",
    "legend.set_title(\"Cluster\", prop={'size':16} )\n",
    "\n",
    "# all this is the same as before\n",
    "ctx.add_basemap(ax, zoom=12, source=ctx.providers.Stamen.TonerLite)\n",
    "ax.set_title('Typology of voting, 2020 General Election', fontsize=20)                           \n",
    "ax.set_ylim([3.98e6, 4.14e6])\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129f0226",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h3>Key Takeaways</h3>\n",
    "<ul>\n",
    "  <li>Cluster analysis is an exploratory data tool</li>\n",
    "  <li>Even if the clusters are pretty self-explanatory, they can be useful</li>\n",
    "  <li>They are an example of <em>data reduction</em>—reducing your data to something that can be readily interpreted</li>\n",
    "  <li>They can be a starting point for further quantitative research—perhaps, use them as a variable in a regression model</li>\n",
    "  <li>They can also be useful for qualitative research. Perhaps you might do a case study of each cluster, picking the precinct/city/agency that is closest to each cluster center</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16260ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
