{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89e060b8",
   "metadata": {},
   "source": [
    "# Natural language processing part 1: \n",
    "# Parsing PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ee7a19",
   "metadata": {},
   "source": [
    "## Lecture objectives\n",
    "* Learn how to load in text data from PDFs\n",
    "* Use `regex` to clean and simplify text data\n",
    "\n",
    "In subsequent notebooks, we'll do the actual text analysis such as topic modeling and sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed883393",
   "metadata": {},
   "source": [
    "## Before you start\n",
    "There is another Python package to install. You can do this from within the Anaconda GUI, but it's easier from the command line as follows:\n",
    "\n",
    "`conda activate uds`\n",
    "\n",
    "`conda install pdfminer.six --channel=conda-forge`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e48689",
   "metadata": {},
   "source": [
    "## Getting the text into Python\n",
    "Before we process any text, we need to take a step back and figure out how to get that text into Python. Typically, plans and other policy documents come as PDFs, which are a pain to read. There are dozens of PDF readers for Python, all of which are flawed in different ways. (See some discussions [here](https://towardsdatascience.com/how-to-extract-text-from-pdf-245482a96de7), [here](https://johannesfilter.com/python-and-pdf-a-review-of-existing-tools/) and [here](https://stackoverflow.com/questions/34837707/how-to-extract-text-from-a-pdf-file).) We'll use `pdfminer.six`, which is fairly robust and is easier to install than some alternatives. YMMV.\n",
    "\n",
    "Let's start with an adaptation of the [LA Times analysis of California High-Speed Rail](https://github.com/datadesk/hsr-document-analysis). If you look at their code, they use the `urllib` library to download files. You can do the same but with a couple of extra steps using `requests`. \n",
    "\n",
    "But for now, let's work with just one of their files: [the EIR section on air quality and climate change, for the Bakersfield to Palmdale segment](https://hsr.ca.gov/wp-content/uploads/docs/programs/bakersfield-palmdale/BP_Draft_EIRS_Vol_1_CH_3.3_Air_Quality_and_Global_Climate_Change.pdf). It's in your git repository.\n",
    "\n",
    "We'll read the text using `pdfminer.six`. Its simplest function is `extract_text`. Full documentation is [here]\n",
    "\n",
    "Note that you will often have to experiment with other PDF parsers if you get unintelligible results. `PyPDF2` is another commonly used package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "687ac868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text is 465131 characters long\n"
     ]
    }
   ],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "fn = 'data/BP_Draft_EIRS_Vol_1_CH_3.3_Air_Quality_and_Global_Climate_Change.pdf'\n",
    "eirtext = extract_text(fn)\n",
    "print('Text is {} characters long'.format(len(eirtext)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ee237c",
   "metadata": {},
   "source": [
    "Let's look at a few random extracts. We read the file into a string, so we can use our standard string slicing syntax.\n",
    "\n",
    "For example, let's look at 1,000 characters starting at the 200,000th character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e79b317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y \n",
      "\n",
      "Bakersfield to Palmdale Project Section Draft Project EIR/EIS  \n",
      "\n",
      "February 2020 \n",
      "\n",
      " Page | 3.3-55 \n",
      "\n",
      " \n",
      " \n",
      "\f",
      "Section 3.3  Air Quality and Global Climate Change \n",
      "\n",
      "Antelope Valley Air Quality Management District \n",
      "\n",
      "Emission inventory data for the AVAQMD for 2012 are summarized in Table 3.3-15. In the \n",
      "AVAQMD, mobile-source emissions account for over 91 percent and 69 percent of the CO and \n",
      "NOX emissions inventory, respectively. Area sources made up over 55 percent of the particulate \n",
      "emissions, whereas stationary sources made up 45 percent of particulate emissions. Mobile \n",
      "sources were 64 percent of the SOX emissions. Stationary sources made up 43 percent of the \n",
      "areawide ROG emissions.  \n",
      "\n",
      "Table 3.3-15 Estimated Annual Average Emissions for the Antelope Valley Air Quality \n",
      "Management District (tons per day)  \n",
      "\n",
      "Source Category \n",
      "\n",
      "TOG \n",
      "\n",
      "ROG \n",
      "\n",
      "CO \n",
      "\n",
      "NOX \n",
      "\n",
      "SOX \n",
      "\n",
      "Particulate \n",
      "Matter \n",
      "\n",
      "PM10 \n",
      "\n",
      "PM2.5 \n",
      "\n",
      "Stationary Sources \n",
      "\n",
      "Fuel Combustion \n",
      "\n",
      "Waste Disposal \n",
      "\n",
      "Cleaning and Surface Coatings \n",
      "\n",
      "Petroleum P\n"
     ]
    }
   ],
   "source": [
    "print(eirtext[200001:201001])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49730a5",
   "metadata": {},
   "source": [
    "And another slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6af34f05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PM2.5 hot-spot analysis, regardless of a medium or high ridership scenario. In December \n",
      "2010, the USEPA released its Transportation Conformity Guidance for Quantitative Hot-spot \n",
      "Analyses in PM2.5 and PM10 Nonattainment and Maintenance Areas (USEPA 2015b), which was \n",
      "used for this analysis. Although this analysis is normally associated with the Transportation \n",
      "Conformity Rule, this project is subject to the General Conformity Rule. The decision to use this \n",
      "analytical structure notwithstanding, additional analysis or associated activities required to comply \n",
      "with Transportation Conformity will be carried out only if discrete project elements become \n",
      "subject to those requirements in the future. In accordance with this guidance, if a project meets \n",
      "one of the following criteria, it is considered a project of air quality concern and a quantitative \n",
      "PM10/PM2.5 analysis is required.  \n",
      "\n",
      "•  New or expanded highway projects that have a significant number of or significant \n",
      "increase in diesel \n"
     ]
    }
   ],
   "source": [
    "print(eirtext[400001:401001])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1f2332",
   "metadata": {},
   "source": [
    "## Cleaning up the text\n",
    "So we've got a bunch of text in, but clearly the formatting leaves something to be desired. In particularly, there are a lot of random line breaks. Let's use `regex` to convert all whitespace (spaces, tabs (`\\t`), and newlines (`\\n` or `\\r\\n`) to a single space. \n",
    "\n",
    "`regex` is short for \"regular expression,\" and is essentially a pattern matching tool for text. Think of it as a souped-up version of `replace`. \n",
    "\n",
    "`regex` is extremely powerful and has an extremely unfriendly syntax. But there are thousands of examples online. [Here's a good place to start](https://regexone.com/) if you want to explore more. And [this website](https://regex101.com) helps you test and debug your expressions.\n",
    "\n",
    "Let's look at an example – `r\"\\s+\"`:\n",
    "- The `r` tells Python that what follows is a \"raw string,\" and thus the `\\` character should be interpreted literally\n",
    "- `\\s` matches whitespace\n",
    "- `+` matches multiple occurences\n",
    "\n",
    "So basically, we are matching all whitespace, however long.\n",
    "\n",
    "Let's then use `re.sub` to replace that whitespace. The second argument is what we replace our matched substrings with. The third argument is the string to apply the substitution to. Note that we have some spaces, some tabs (`\\t`), and some newlines (`\\n`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c1ece90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HSR is an expensive boondoogle\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "print(re.sub(r\"\\s+\", \" \", \"HSR\\tis     an\\nexpensive    boondoogle\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc138192",
   "metadata": {},
   "source": [
    "If we omit the `+` and just specify `r\"\\s\"`, we don't match multiple occurences. So 4 spaces are replaced with 4 spaces, rather than a single space. But the tabs and newlines are still converted to spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03a03838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HSR will      transform     California\n"
     ]
    }
   ],
   "source": [
    "print(re.sub(r\"\\s\", \" \", \"HSR\\twill     \\ntransform     California\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c89c5a1",
   "metadata": {},
   "source": [
    "I won't pass judgment on the content of either of these claims.\n",
    "\n",
    "Let's apply the `regex` to our text that we pulled out of the EIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59d45429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-Speed Rail Authority Bakersfield to Palmdale Project Section Draft Project EIR/EIS Section 3.3 Air Quality and Global Climate Change Figure 3.3-3 Sensitive Receptors within the High-Speed Rail Project Vicinity California High-Speed Rail Authority Bakersfield to Palmdale Project Section Draft Project EIR/EIS (Sheet 1 of 11) February 2020 Page | 3.3-59 Section 3.3 Air Quality and Global Climate Change Figure 3.3-3 Sensitive Receptors within the High-Speed Rail Project Vicinity (Sheet 2 of 11) February 2020 3.3-60 | Page California High-Speed Rail Authority Bakersfield to Palmdale Project Section Draft Project EIR/EIS Section 3.3 Air Quality and Global Climate Change Figure 3.3-3 Sensitive Receptors within the High-Speed Rail Project Vicinity California High-Speed Rail Authority Bakersfield to Palmdale Project Section Draft Project EIR/EIS (Sheet 3 of 11) February 2020 Page | 3.3-61 Section 3.3 Air Quality and Global Climate Change Figure 3.3-3 Sensitive Receptors within the High-Speed R\n"
     ]
    }
   ],
   "source": [
    "eirtext = re.sub(r\"\\s+\", \" \", eirtext)\n",
    "print(eirtext[200001:201001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba76105",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eirtext[400001:401001])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1a66fa",
   "metadata": {},
   "source": [
    "We can also use `regex` to get rid of punctuation, digits, etc. \n",
    "\n",
    "Here:\n",
    "* `[]` means match anything within the brackets\n",
    "* `^` means not\n",
    "* `A-z` is any letter in any case\n",
    "* `\\s` is any whitespace (which is just spaces, since we converted other whitespace like tabs to spaces\n",
    "\n",
    "So `[^A-z\\s]` captures anything that is not a letter or whitespace. \n",
    "\n",
    "Since we might want the punctuation at a later date, let's assign our cleaned text to a new variable, `eirtext_wordsonly`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73d0572e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d avoid microscale CO impacts and localized PMPM hotspot impacts It is also anticipated that all of the BP Build Alternatives would avoid localized air quality impacts to sensitive receptors including schools All of the BP Build Alternatives would also avoid impacts related to other emissions such as those leading to odors adversely affecting a substantial number of people All of the BP Build Alternatives would avoid impacts related to compliance with applicable air quality plans during project operation and would result in anticipated net reduction in criteria pollutant and GHG emissions within the SJVAB and the MDAB All of the BP Build Alternatives would avoid cumulative impacts during project operation and would result in anticipated net reduction in criteria pollutant and GHG emissions within the SJVAB and the MDAB  CEQA Significance Conclusions Table  provides a summary of the CEQA determination of significance for all construction and operations impacts discussed in Section  If t'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eirtext_wordsonly = re.sub(r\"[^A-z\\s]\", \"\", eirtext)\n",
    "eirtext_wordsonly[400001:401001]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94da52e7",
   "metadata": {},
   "source": [
    "Notice that removing some digits, etc. means that we now have extra spaces. For example, `Table 3.3-46 provides a summary` becomes `Table  provides a summary.`\n",
    "\n",
    "So let's use our same process from before to remove duplicate spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e50b438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CEQA Significance Conclusions Table provides a summary of the CEQA determination of significance for all construction and operations impacts discussed in Section If there are differences in impacts before or after mitigation among the BP Build Alternatives these are noted in the table Where there is no differ'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eirtext_wordsonly = re.sub(r\"\\s+\", \" \", eirtext_wordsonly)\n",
    "eirtext_wordsonly[394890:395200]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca56d529",
   "metadata": {},
   "source": [
    "This looks much better! We now have some clean text to analyze.\n",
    "\n",
    "Let's pause here. We'll save the text to a file, so that we can load it in at the start of the next lecture.\n",
    "\n",
    "Note here that `open` opens the file object `f`. We then write `eirtext` to the file. The `with` syntax helps because it automatically closes the file afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46750056",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('eirtext.txt', 'w') as f:\n",
    "    f.write(eirtext)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b453f388",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h3>Key Takeaways</h3>\n",
    "<ul>\n",
    "  <li>PDFs are difficult to work with. pdfminer is a good starting point, but make sure to inspect your output.</li>\n",
    "  <li>regex is a powerful tool to clean up text, e.g. removing whitespace and punctuation.</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4240a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
