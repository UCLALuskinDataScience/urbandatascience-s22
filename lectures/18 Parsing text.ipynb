{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89e060b8",
   "metadata": {},
   "source": [
    "# Natural language processing part 2:\n",
    "# Parsing text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ee7a19",
   "metadata": {},
   "source": [
    "## Lecture objectives\n",
    "* Learn how to do simple word counts\n",
    "* Understand how to tokenize (split) text into words and sentences\n",
    "* Understand how and when to lemmatize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e48689",
   "metadata": {},
   "source": [
    "In the previous lecture, we parsed the PDF and obtained a clean string. Now, we can do something with that string that requires an understanding of the language - i.e., natural language processing.\n",
    "\n",
    "We'll focus on English-language parsing. Many of the principles are applicable to other languages too.\n",
    "\n",
    "First, let's load in the text file that we created in the previous lecture. It's the same `open` function, but we'll use `r` (read) rather than `w` (write)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687ac868",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('eirtext.txt','r') as f:\n",
    "    eirtext = f.read()\n",
    "print(len(eirtext))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a576351a",
   "metadata": {},
   "source": [
    "## Bags of words\n",
    "One of the simplest ways to analyze a text is to look at the raw count of words.\n",
    "\n",
    "Let's write a function to take a piece of text, and return a DataFrame with word counts. \n",
    "\n",
    "What do we do? First, create an empty dictionary to hold our counts. The keys will be the words, and the values the counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fb06c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f378d1fb",
   "metadata": {},
   "source": [
    "Then, let's split our text into a list of words. `split()` splits a string by any character - the default is a space. \n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af492098",
   "metadata": {},
   "outputs": [],
   "source": [
    "'This is a sentence'.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7419a0",
   "metadata": {},
   "source": [
    "Or split by the hyphens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7f3d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "'This-is-a-sequence of words'.split('-')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedb7d86",
   "metadata": {},
   "source": [
    "So now let's do that with our EIR text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d74aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist = eirtext.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4bb4f9",
   "metadata": {},
   "source": [
    "We have a list of words - `wordlist`. Let's loop over them.\n",
    "\n",
    "If that word already exists in our dictionary, we add 1 to the value. Otherwise, we create a new key and a value of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6509dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in wordlist:\n",
    "    lword = word.lower() # convert to lowercase\n",
    "    if lword in counts:\n",
    "        counts[lword] +=1\n",
    "    else:\n",
    "        # doesn't exist in the dictionary\n",
    "        counts[lword] = 1\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a217468",
   "metadata": {},
   "source": [
    "Let's convert this dictionary to a DataFrame, sort it by the word count column, and add a name to the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b2bf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame.from_dict(counts, orient='index', columns=['word_count'])\n",
    "df.sort_values('word_count', ascending=False, inplace=True)\n",
    "df.index.name = 'word'\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ac0c16",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Exercise:</strong> Turn the preceding steps into a function. The argument should be a list of words, and the function should return a DataFrame.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092d8664",
   "metadata": {},
   "source": [
    "Here, we just take all the code above, indent it, and give the function a name (`countWords`), an argument (`wordlist`), and a `return` statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cca449e",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def countWords(wordlist):\n",
    "    counts = {} \n",
    "    \n",
    "    for word in wordlist:\n",
    "        lword = word.lower()\n",
    "        if lword in counts:\n",
    "            counts[lword] +=1\n",
    "        else:\n",
    "            counts[lword] = 1\n",
    "\n",
    "    df = pd.DataFrame.from_dict(counts, orient='index', columns=['word_count'])\n",
    "    df.sort_values('word_count', ascending=False, inplace=True)\n",
    "    df.index.name = 'word'\n",
    "    \n",
    "    return df\n",
    "\n",
    "wordlist = eirtext.split()\n",
    "df = countWords(wordlist)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8025cbb",
   "metadata": {},
   "source": [
    "## Tokenizing and stopwords\n",
    "I suppose it's good that an EIR section on air quality mentions emissions. But the other words aren't particularly informative. This type of analysis might be useful in some applications, but here, we really need to push further.\n",
    "\n",
    "Let's use the `nltk` library to get rid of the little words like \"the,\" \"for,\" etc. These are called *stop words* in natural language processing jargon. \n",
    "\n",
    "`nltk` is a mammoth library, and has lots of submodules. We'll use the tokenize functions (more on this in a moment) and `stopwords` submodules for now.\n",
    "\n",
    "The first time we use them, we have to download the \"corpus\". If you don't do this, you'll get a helpful error message reminding you of this. See http://www.nltk.org/nltk_data/ for all the corpora that you can download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235d26dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# We only need to do this once \n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db7ad98",
   "metadata": {},
   "source": [
    "Let's take a look at the stopwords. `stopwords.words` just gives us a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381ab022",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9590a961",
   "metadata": {},
   "source": [
    "In several languages, too. Let's look at the first 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b4521c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stopwords.words('spanish')[:10])\n",
    "print(stopwords.words('arabic')[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4f4f03",
   "metadata": {},
   "source": [
    "Tokenizing functions are essentially splitting function. For example, `sent_tokenize` splits into a list of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f985c31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokenize(eirtext)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7097ca",
   "metadata": {},
   "source": [
    "For our purposes, we want to split into words. We can use `word_tokenize`. This should give us similar results to the `split()` function earlier, but it's a bit more robust.\n",
    "\n",
    "Before we count the words, let's also use `regex` to drop the digits, punctuation, and other non-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f27a932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "wordlist = word_tokenize(re.sub(r\"[^A-z\\s]\", \"\", eirtext))\n",
    "df = countWords(wordlist)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61008831",
   "metadata": {},
   "source": [
    "Now let's drop the stopwords from our counts. \n",
    "\n",
    "Remember that `stopwords.words` gives a list of words. So let's use the pandas `drop()` function to drop all of those words from the index. \n",
    "\n",
    "We add the `errors='ignore'` argument because not all of our stopwords will be in our DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1813cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(index=stopwords.words('english'), errors='ignore', inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284819ec",
   "metadata": {},
   "source": [
    "## Lemmatizing\n",
    "Finally, we might want to *lemmatize* the words. We saw that process used in the [Brinkley & Stahmer](https://journals.sagepub.com/doi/abs/10.1177/0739456X21995890) paper. Lemmatization groups words with the same stem, e.g. `highway` and `highways`, or `constructing` and `construction`, through reducing them to their *root*.\n",
    "\n",
    "`nltk` has a built-in function for that - `PorterStemmer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074863c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "print(ps.stem('construction'))\n",
    "print(ps.stem('highways'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a74fa0",
   "metadata": {},
   "source": [
    "Even if it doesn't know the (made-up) word, the stemmer takes a decent guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aaf2b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ps.stem('housingelementifcation'))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bd64ac",
   "metadata": {},
   "source": [
    "Let's add this to our function, and call our new function `countStems`. It's just one extra line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e14d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def countStems(wordlist):\n",
    "    counts = {} \n",
    "    \n",
    "    for word in wordlist:\n",
    "        lword = word.lower()\n",
    "        # This is the extra line\n",
    "        lword = ps.stem(lword)\n",
    "        \n",
    "        if lword in counts:\n",
    "            counts[lword] +=1\n",
    "        else:\n",
    "            counts[lword] = 1\n",
    "\n",
    "    df = pd.DataFrame.from_dict(counts, orient='index', columns=['word_count'])\n",
    "    df.sort_values('word_count', ascending=False, inplace=True)\n",
    "    df.index.name = 'word'\n",
    "    df.drop(index=stopwords.words('english'), errors='ignore', inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = countStems(wordlist)\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dc1d53",
   "metadata": {},
   "source": [
    "Whether the stems are more useful than the original words is obviously a matter for your specific task.\n",
    "\n",
    "So now we've got the tools to bring in some text to a useful form. In the next lectures, we'll interpret the text using topic modeling and sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b453f388",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h3>Key Takeaways</h3>\n",
    "<ul>\n",
    "  <li>Before analyzing a text, you will probably need to do clean-up such as removing stopwords, converting to lower case, and possibly lemmatizing the words.</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4240a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
