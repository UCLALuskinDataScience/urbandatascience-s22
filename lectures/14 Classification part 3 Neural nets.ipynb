{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4995c060",
   "metadata": {},
   "source": [
    "# Machine learning: Classification\n",
    "## Part 3. Neural networks and logistic regression\n",
    "## Lecture objectives\n",
    "\n",
    "1. Learn how to estimate neural network models\n",
    "2. Learn how to estimate logistic regression models\n",
    "3. More practice with train-test splits and assessing model performance\n",
    "4. Learn how to standardize data\n",
    "\n",
    "In the previous video lecture, we estimated a random forests model using `scikit-learn`. \n",
    "\n",
    "Here, we'll explore other machine learning algorithms.\n",
    "\n",
    "Most have almost identical syntax, meaning once you are familiar with one model it's easy to apply another model. However, they will have different hyperparameters, such as the number of trees in the random forest.\n",
    "\n",
    "To start with, let's do the following:\n",
    "* load the data we previous saved as a pickle\n",
    "* recreate the dummy variables\n",
    "* create a dataframe with the subset of variables that we want to use, and drop the NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dd46dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the same code as from the last lecture\n",
    "import pandas as pd\n",
    "joinedDf = pd.read_pickle('joined_permits.pandas')\n",
    "\n",
    "dummies1 = pd.get_dummies(joinedDf.UseType, prefix='usetype_')  # creates a dataframe of dummies\n",
    "dummies2 = pd.get_dummies(joinedDf.UseDescription, prefix='usedesc_')\n",
    "joinedDf = joinedDf.join(dummies1).join(dummies2) \n",
    "\n",
    "xvars = (dummies1.columns.tolist() + dummies2.columns.tolist() + \n",
    "            ['YearBuilt1', 'Units1', 'Bedrooms1', 'Bathrooms1', 'SQFTmain1', \n",
    "             'Roll_LandValue', 'Roll_ImpValue', 'Roll_LandBaseYear', \n",
    "             'Roll_ImpBaseYear', 'CENTER_LAT', 'CENTER_LON' ])\n",
    "yvar = 'hasADU'\n",
    "\n",
    "# create a dataframe with no NaNs\n",
    "df_to_fit = joinedDf[xvars+[yvar]].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6e94cd",
   "metadata": {},
   "source": [
    "Let's also import the relevant `scikit-learn` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e011e347",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fd3f0a",
   "metadata": {},
   "source": [
    "## Standardizing data\n",
    "Many machine learning algorithms are more robust if we *standardize* the data - subtract the mean and divide by the standard deviation. This puts each variable on a common scale.\n",
    "\n",
    "It didn't matter for random forests, but it does for neural networks.\n",
    "\n",
    "Let's do this. Note that we need to exclude the dummy variable columns and the dependent variable.\n",
    "\n",
    "To identify them, we'll use a Python [list comprehension](https://www.w3schools.com/python/python_lists_comprehension.asp). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b65f119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: add a suffix to each column\n",
    "oldcols = ['units', 'squarefeet','lotsize']\n",
    "newcols = []\n",
    "for col in oldcols:\n",
    "    newcols.append(col+'_2020')\n",
    "print(newcols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bc2bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is much cleaner as a list comprehension\n",
    "[col+'_2020' for col in oldcols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd01524",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_exclude = [col for col in df_to_fit.columns \n",
    "                if col.startswith('usetype_') or col.startswith('usedesc_') \n",
    "                or col=='hasADU']\n",
    "\n",
    "# this is the same as\n",
    "cols_to_exclude = []\n",
    "for col in df_to_fit.columns:\n",
    "    if col.startswith('usetype_') or col.startswith('usedesc_') or col=='hasADU':\n",
    "        cols_to_exclude.append(col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b55ab5",
   "metadata": {},
   "source": [
    "Now let's create a list of the columns that we *don't* want to exclude. \n",
    "\n",
    "We can use a list comprehension again: we'll include that column in the new list if the condition (`col not in cols_to_exclude`) is `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669ee4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "otherCols = [col for col in df_to_fit.columns if col not in cols_to_exclude]\n",
    "otherCols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7385a31d",
   "metadata": {},
   "source": [
    "Now let's scale `otherCols`. Note that the `StandardScaler` returns a numpy array, not a pandas DataFrame. So we need to convert the array to a dataframe and specify the column names and the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add4105a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see https://scikit-learn.org/stable/modules/preprocessing.html for standardization\n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler().fit(df_to_fit[otherCols])\n",
    "\n",
    "# convert to DataFrame and specify the column names and index\n",
    "df_scaled = pd.DataFrame(scaler.transform(df_to_fit[otherCols]), \n",
    "                         columns=otherCols, index=df_to_fit.index)\n",
    "\n",
    "# create a DataFrame with these scaled columns joined to the columns that we didn't scale\n",
    "df_scaled = df_scaled.join(df_to_fit[cols_to_exclude])\n",
    "\n",
    "df_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0676a0d9",
   "metadata": {},
   "source": [
    "We can see that the standardization works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba33fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef008d8",
   "metadata": {},
   "source": [
    "We'll do our train/test split as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae29fe03",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "      df_scaled[xvars], df_scaled[yvar], test_size = 0.25, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33657598",
   "metadata": {},
   "source": [
    "And estimate our neural network model. \n",
    "\n",
    "Note that the workflow and syntax is very similar to the random forests:\n",
    "* Initialize the classifier object - here, we call it `mlp`\n",
    "* Fit to the data\n",
    "* Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02889322",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(10, 10, 10), max_iter=1000)\n",
    "mlp.fit(X_train, y_train)\n",
    "y_pred = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94361499",
   "metadata": {},
   "source": [
    "How did we do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f582aade",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d450b8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e65db07",
   "metadata": {},
   "source": [
    "Interestingly, we get very similar results to the random forests. Perhaps this indicates the inherent unpredictability of ADUs, given how rarely they are constructed. Or we might be able to do better with additional predictors or through adjusting the hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5fd413",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "As a point of comparison, how would a more traditional logistic regression fare?\n",
    "\n",
    "Many different regression estimators are implemented in `scikit-learn`. And the syntax should be familiar by now. Note that standardization (as we did for neural networks) helps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61237a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cef0fc",
   "metadata": {},
   "source": [
    "Note that it doesn't even converge! Methods like logistic regression don't handle highly correlated variables very well.\n",
    "\n",
    "We might be able to do better with a smaller set of predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6120ad5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xvars = ['YearBuilt1', 'Units1', 'Bedrooms1', 'Bathrooms1', 'SQFTmain1', 'Roll_LandValue', \n",
    "             'Roll_ImpValue', 'Roll_LandBaseYear', 'Roll_ImpBaseYear', 'CENTER_LAT', 'CENTER_LON', 'usedesc__Single']\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train[xvars], y_train)\n",
    "y_pred = lr.predict(X_test[xvars])\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c65f60",
   "metadata": {},
   "source": [
    "Not so great, eh? So our random forests and neural networks approaches look much better by comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93785bc8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h3>Key Takeaways</h3>\n",
    "<ul>\n",
    "  <li>There are several different approaches to machine learning. Random forests and neural networks are two of the most popular.</li>\n",
    "  <li>scikit-learn provides a consistent syntax: initialize-fit-predict. So once you've done one ML model, others are much simpler.</li>\n",
    "  <li>Confusion matrices are an excellent way to assess predictive performance.</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000f0942",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
