{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7853aaf0",
   "metadata": {},
   "source": [
    "# Natural language processing part 5:\n",
    "# Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9e9f6b",
   "metadata": {},
   "source": [
    "## Lecture objectives\n",
    "* Learn how to implement and interpret sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3505057a",
   "metadata": {},
   "source": [
    "In its simplest form, sentiment analysis works through applying a corpus of words and phrases that indicate sentiment. [SentiWordNet](https://github.com/aesuli/SentiWordNet) is a commonly used corpusâ€”[look at the list here](https://raw.githubusercontent.com/aesuli/SentiWordNet/master/data/SentiWordNet_3.0.0.txt). For example, \"worst,\" \"terrible,\" and \"apprehensive\" have negative scores, while \"feel_like_a_million_dollars\" has a positive score. Some words have both positive and negative scores. Some algorithms consider the part of speech in which the word occurs (e.g. is it an adjective or noun).\n",
    "\n",
    "Let's start through loading in the tweets that we saved in the previous video lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f053c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('data/tweets/miami.pickle', 'rb') as f:\n",
    "    miami = pickle.load(f)\n",
    "with open('data/tweets/chicago.pickle', 'rb') as f:\n",
    "    chicago = pickle.load(f)\n",
    "with open('data/tweets/toronto.pickle', 'rb') as f:\n",
    "    toronto = pickle.load(f)\n",
    "miami[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b7b811",
   "metadata": {},
   "source": [
    "Let's turn to sentiment analysis. `textblob` uses the corpora (basically, a body of text) from the `nltk` library. We already downloaded a couple of corpora such as stop words, but we need two more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4302379c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('sentiwordnet')\n",
    "nltk.download('wordnet')\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe569d1",
   "metadata": {},
   "source": [
    "Let's try some examples. Note from the [documentation](https://textblob.readthedocs.io/en/dev/quickstart.html#sentiment-analysis): \n",
    "\n",
    "* The sentiment property returns a named tuple of the form `Sentiment(polarity, subjectivity)`. \n",
    "* The polarity score is a float within the range [-1.0, 1.0].\n",
    "* The subjectivity is a float within the range [0.0, 1.0] where 0.0 is very objective and 1.0 is very subjective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc349440",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = TextBlob('I love riding public transit')\n",
    "print(sentence.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5a7006",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = TextBlob('My bus was late AGAIN today.')\n",
    "print(sentence.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24086578",
   "metadata": {},
   "source": [
    "We can access the polarity score directly. (Let's ignore subjectivity.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c9aa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence.sentiment.polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5814ea",
   "metadata": {},
   "source": [
    "Now let's come back to our tweets. We can compute the sentiment (polarity) score for each tweet. \n",
    "\n",
    "Let's use a list comprehension to loop over each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c4320c",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# the list comprehension is the same as\n",
    "miami_sentiment = []\n",
    "for tweet in miami:\n",
    "    miami_sentiment.append(TextBlob(tweet).sentiment.polarity)\n",
    "\n",
    "miami_sentiment = [TextBlob(tweet).sentiment.polarity for tweet in miami]\n",
    "chicago_sentiment = [TextBlob(tweet).sentiment.polarity for tweet in chicago]\n",
    "toronto_sentiment = [TextBlob(tweet).sentiment.polarity for tweet in toronto]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f45de4b",
   "metadata": {},
   "source": [
    "We get lists of sentiments. Let's look at the first few."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a32b114",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print('Sentiment: {:.2f}. Tweet: {}'.format(miami_sentiment[i], miami[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911c99f5",
   "metadata": {},
   "source": [
    "Now let's visualize it. A histogram seems appropriate here.\n",
    "\n",
    "Note that the seaborn `histplot` can take a list or any other sequence, as well as a `DataFrame`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3963a4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "help(sns.histplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d530348",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(miami_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00157e39",
   "metadata": {},
   "source": [
    "Let's compare the different transit agencies.\n",
    "\n",
    "We've already used the `plt.subplots()` function to create a set of axes. However, its real power comes in creating figures with multiple plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab453e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# this creates a 1x3 matrix of plots, and returns a list of axes objects\n",
    "fig, axes = plt.subplots(1, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a98491",
   "metadata": {},
   "source": [
    "Because `axes` is a list, we can now access each axis as `axes[0]`, `axes[1]`, etc.\n",
    "\n",
    "We can even loop over each axis as with any other list. That's useful for plotting lots of subgroups.\n",
    "\n",
    "Here, we'll use a loop to clean up each plot. We'll use the `zip` notation, that loops over equal-length lists and pairs them up. For example, the first iteration of the loop will put the first element of `axes` in `ax`, and the first element of `cities` in `city`.\n",
    "\n",
    "We'll make the axes have the same extent for each plot, and set the y-axis label on only the left-hand axis.\n",
    "\n",
    "`fig.tight_layout()` is a useful command to clean up the spacing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91027146",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3)\n",
    "\n",
    "sns.histplot(miami_sentiment, ax=axes[0])\n",
    "sns.histplot(chicago_sentiment, ax=axes[1])\n",
    "sns.histplot(toronto_sentiment, ax=axes[2])\n",
    "\n",
    "cities = ['miami', 'chicago', 'toronto']\n",
    "for ax, city in zip(axes, cities):\n",
    "    ax.set_title(city)\n",
    "    ax.set_ylim(0,50)\n",
    "    ax.set_xlim(-1,1)\n",
    "    ax.set_ylabel('')\n",
    "axes[0].set_ylabel('Number of tweets')\n",
    "axes[1].set_xlabel('Sentiment')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc4a484",
   "metadata": {},
   "source": [
    "There is lots more we can do here. For example:\n",
    "* We relied on a single search term. We might expand this (perhaps just \"transit\" rather than \"public transit,\" and add \"bus\" and \"train\" as well. We might also consider adding the name of specific agency such as CTA. Note that generic terms are harder. For example, \"Lyft\" is easier to use as a search term, but \"Uber\" might return a lot of unrelated results.\n",
    "* We could tokenize (split) each tweet into sentences. Otherwise, for longer tweets, the more polarized (opinionated) tweets might be watered down with other sentences.\n",
    "* We could use a different sentiment analyzer ([`TextBlob` has a couple of pre-trained options](https://textblob.readthedocs.io/en/dev/advanced_usage.html)) or train our own sentiment analyzer using the `nltk` \n",
    "functionality. \n",
    "* Note that sentiment analyzers are often trained on movie reviews and similarly \"opinionated\" corpuses, and so more specialist applications need custom training. In some of my [own work](https://conbio.onlinelibrary.wiley.com/doi/10.1111/csp2.624), we found that the writing was too technical or dry in style for sentiment analysis to work.\n",
    "\n",
    "But we'll leave those for you to explore on your own.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ce3eca",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h3>Key Takeaways</h3>\n",
    "<ul>\n",
    "    <li>Sentiment analysis can identify positive and negative sentiments towards a topic. The pre-trained models might not work well for your data, but you can train your own.</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5161b51a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
