{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53fce03f",
   "metadata": {},
   "source": [
    "## Week 6 Class activities\n",
    "This notebook is a starting point for the exercises and activities that we'll do in class. We'll do an extension of the random forests classifier, looking at a continuous variable. Then, we'll do some cluster analysis.\n",
    "\n",
    "Before you attempt any of these activities, make sure to watch the video lectures for this week."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c092609b",
   "metadata": {},
   "source": [
    "### Classification: NYC evictions\n",
    "We'll look at the factors that are associated with evictions in New York City. Perhaps a machine learning model can identify the types of places that are vulnerable to eviction, and target renter assistance programs more effectively?\n",
    "\n",
    "#### Loading in the data\n",
    "\n",
    "Let's start by loading in the [eviction dataset](https://data.cityofnewyork.us/City-Government/Evictions/6z8x-wfk4) via Socrata."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30faed6",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<strong>Exercise:</strong> Import the data from Socrata via the API into a pandas DataFrame.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479b86bd",
   "metadata": {},
   "source": [
    "*Hints*:\n",
    "- Look back at Week 1 if you need a refresher on using Socrata\n",
    "- There are about 70,000 rows in the dataset. So remember to add `?$limit=100000` to the end of the URL that you pass to `requests.get()`. Otherwise, you'll just get the first 1,000 rows. (The limit can be anything comfortably above 70000.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4133f155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# your code here\n",
    "\n",
    "url = 'https://data.cityofnewyork.us/resource/6z8x-wfk4.json?$limit=100000'\n",
    "r = requests.get(url)\n",
    "df = pd.DataFrame(json.loads(r.content))\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9549f5a2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<strong>Exercise:</strong> Convert your dataframe to a GeoDataFrame, using the latitude and longitude columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e02f9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here \n",
    "gdf = gpd.GeoDataFrame(df, \n",
    "            geometry = gpd.points_from_xy(\n",
    "                df.longitude, df.latitude, crs='EPSG:4326'))\n",
    "gdf.plot() # make sure it looks ok"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5657ee",
   "metadata": {},
   "source": [
    "Now let's import some census data. We could use `cenpy` or the Census Bureau API. But to keep things simple so that we can focus on the spatial joins and the machine learning, I downloaded the block group-level 2019 ACS data for New York from the [Census Bureau](https://www.census.gov/geographies/mapping-files/time-series/geo/tiger-data.html). To save space, I clipped it to the 5 NYC counties.\n",
    "\n",
    "It's in your repository, and we can load it in as follows. If you aren't familiar with a GeoPackage (GPKG) format, think of it as a \"new and improved shapefile.\" [Here's a good overview.](https://towardsdatascience.com/why-you-need-to-use-geopackage-files-instead-of-shapefile-or-geojson-7cb24fe56416)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d4aef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bgs = gpd.read_file('data/nyc_bgs.gpkg')\n",
    "bgs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c295e3d",
   "metadata": {},
   "source": [
    "Note that the variables aren't particularly carefully selected - I just threw in many of the demographic and housing variables. \n",
    "\n",
    "Nor are the variable names particularly informative, but the full names are in a file in the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86627295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note it is tab-sepated, not comma separated\n",
    "# so we use the sep='\\t' argument\n",
    "\n",
    "col_names = pd.read_csv('data/BG_METADATA_2019.txt', sep='\\t', index_col='Short_Name')\n",
    "col_names.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4ea2c7",
   "metadata": {},
   "source": [
    "So you can see the definition of the column like this. (I don't recommend renaming the `bg` column names, because the full names are so long.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f59fcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names.loc['B01001e1']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da707b5",
   "metadata": {},
   "source": [
    "#### Spatial join\n",
    "Now let's do the spatial join. Again, let's follow our three step process.\n",
    "\n",
    "1. Use a spatial join to add the `GEOID` column to the evictions dataframe. *Hint:* Check your projections.\n",
    "2. Group by `GEOID` to get a count of evictions per block group. If you have a `Series`, give it a name - maybe `n_evictions`\n",
    "3. Join those counts back - a tabular join based on the index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accd0b18",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <strong>Exercise:</strong> Add a count of evictions per census block group to your <strong>bgs</strong> GeoDataFrame, using the 3-step process above.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc447f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "# spatial join\n",
    "# match the CRS\n",
    "bgs.to_crs('EPSG:4326', inplace=True)\n",
    "\n",
    "gdf2 = gpd.sjoin(gdf, bgs, predicate='intersects')\n",
    "\n",
    "# count \n",
    "n_evictions = gdf2.groupby('GEOID').size()\n",
    "n_evictions.name = 'n_evictions'\n",
    "\n",
    "# join back\n",
    "bgs = bgs.set_index('GEOID').join(n_evictions)\n",
    "bgs.n_evictions.fillna(0, inplace=True)\n",
    "bgs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89523f4a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Exercise:</strong> Do a quick-and-dirty map of the number of evictions. This will help identify any data holes.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2991b6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bgs.plot('n_evictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78b5b4f",
   "metadata": {},
   "source": [
    "#### Random forests regressor\n",
    "Now we have our data set. Let's estimate a random forests model.\n",
    "\n",
    "In contrast to the previous examples, we are trying to predict a continuous variable - the number of evictions. So our classifier isn't appropriate. \n",
    "\n",
    "However, there is a similar model: the [random forest regressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor). It works almost identically to the classifier. The main difference from a user perspective is assessing model performance - a confusion matrix doesn't work here.\n",
    "\n",
    "You'll need to follow the following steps:\n",
    "- choose your x variables. (Your y variable will be `n_evictions`)\n",
    "- Drop Null values if needed\n",
    "- split your dataset into training and testing portions\n",
    "- estimate (fit) the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24327cee",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Exercise:</strong> Estimate a random forest regressor model to predict the number of evictions per census tract.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192468ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# your code here\n",
    "# I'm just going to throw in all of the columns\n",
    "xvars = [col for col in bgs.columns if col not in ['geometry','n_evictions']]\n",
    "yvar = 'n_evictions'\n",
    "\n",
    "# create a dataframe with no NaNs\n",
    "df_to_fit = bgs[xvars+[yvar]].dropna()\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_to_fit[xvars], df_to_fit[yvar], test_size = 0.25, random_state = 1)\n",
    "\n",
    "# initialize the random forest classifer object\n",
    "rf = RandomForestRegressor(n_estimators = 50, random_state = 1)\n",
    "\n",
    "# now fit the model\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "y_pred = rf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5483eb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Exercise:</strong> Assess the fit of your model.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aded99af",
   "metadata": {},
   "source": [
    "Remember, the confusion matrix and accuracy scores don't apply here. Some ideas for continuous variables are [here](https://stackoverflow.com/questions/50789508/random-forest-regression-how-do-i-analyse-its-performance-python-sklearn). You could also plot actual vs predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779701e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# I used mean absolute error\n",
    "print(metrics.mean_absolute_error(y_test, y_pred))\n",
    "\n",
    "# but the scatter plot is more helpful to me\n",
    "fig, ax = plt.subplots()\n",
    "sns.regplot(x=y_test, y=y_pred, ax=ax)\n",
    "ax.set_xlabel('Actual evictions')\n",
    "ax.set_ylabel('Predicted evictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f5e2a1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Exercise:</strong> Which variables are most important in your predictions? Plot the forest importances.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d83e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "# code from https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "importances = rf.feature_importances_\n",
    "\n",
    "# convert to a series, and give the index labels from our X_train dataframe\n",
    "forest_importances = pd.Series(importances, index=X_train.columns)\n",
    "\n",
    "# get the standard deviations to be able to plot the error bars\n",
    "std = np.std([tree.feature_importances_ for tree in rf.estimators_], axis=0)\n",
    "\n",
    "# sort the importances in descending order\n",
    "forest_importances.sort_values(inplace=True, ascending=False)\n",
    "\n",
    "# let's plot just the top 10\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4,10))\n",
    "sns.barplot(x=forest_importances.values[:10], y=forest_importances.index[:10], yerr=std[:10], ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "\n",
    "# let's find out what those variable are\n",
    "# it would be better to actually change the axis tick label, but this is quick and dirty\n",
    "\n",
    "for col_name in forest_importances.index.values[:10]:\n",
    "    print(col_names.loc[col_name].values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518387dd",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "We'll now look at some k-means cluster analysis. The question: are there particular patterns of cruising for parking? You can see [my version of the analysis here](https://findingspress.org/article/28061-the-shape-of-cruising), joint with Robert Hampshire and Rachel Weinberger.\n",
    "\n",
    "The data file that replicate the analysis is in your data folder. There is one row for each cruising trip (derived from the final portion of a GPS trace, once a driver is assumed to start looking for parking.)\n",
    "\n",
    "You can load the data as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ab8e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "cruisingDf = pd.read_csv('data/cruising_shapes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09baf9bb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Exercise:</strong> Replicate the analysis in Millard-Ball, Hampshire and Weinberger (2021). First, add each cluster label to the dataframe.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a79648a",
   "metadata": {},
   "source": [
    "In the paper, we used 5 clusters and the following columns: `'matchdist', 'frc_repeat', 'n_crossings', 'convexhull_ratio', 'frc_right', 'frc_left', 'frc_uturn', 'frc_straight', 'n_turns'`\n",
    "\n",
    "* `matchdist` is path length (technically, the map-matched distance)\n",
    "* `frc_repeat` is the fraction of repeated blocks (a driver drives on them more than once while cruising for parking)\n",
    "* `n_crossings` is the number of times that the driver crosses over their path\n",
    "* `convexhull_ratio` is a measure of the compactness of the search area\n",
    "* `frc_right`, `frc_left`, `frc_uturn` and `frc_straight` are the fraction of times that the driver turns right or left, makes a U-turn, or continues straight at an intersection\n",
    "* `n_turns` is the number of turns in the cruising trace\n",
    "\n",
    "You'll need to:\n",
    "* standardize the variables\n",
    "* drop Null values\n",
    "* run the k-means algorithm\n",
    "* add the cluster labels back to your dataframe\n",
    "\n",
    "How many observations do you get in each cluster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc7705e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# your code here\n",
    "cols = ['matchdist', 'frc_repeat', 'n_crossings', 'convexhull_ratio', 'frc_right', 'frc_left', 'frc_uturn', 'frc_straight', 'n_turns']\n",
    "scaler = preprocessing.StandardScaler().fit(cruisingDf[cols])\n",
    "df_scaled = pd.DataFrame(scaler.transform(cruisingDf[cols]), \n",
    "                         columns=cols, index=cruisingDf.index)\n",
    "\n",
    "kmeans = KMeans(n_clusters=5, random_state=1).fit(df_scaled)\n",
    "\n",
    "df_scaled['cluster_id'] = kmeans.labels_\n",
    "df_scaled.groupby('cluster_id').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12b299e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Exercise:</strong> Create a radar plot of your clusters.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9132fa03",
   "metadata": {},
   "source": [
    "Here's the function to create the radar chart, which we used in the video lecture.\n",
    "\n",
    "It takes two arguments: the `kmeans` object created by `KMeans`, and your standardized dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185c92ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from https://matplotlib.org/stable/gallery/specialty_plots/radar_chart.html\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Circle, RegularPolygon\n",
    "from matplotlib.path import Path\n",
    "from matplotlib.projections.polar import PolarAxes\n",
    "from matplotlib.projections import register_projection\n",
    "from matplotlib.spines import Spine\n",
    "from matplotlib.transforms import Affine2D\n",
    "\n",
    "\n",
    "def radar_factory(num_vars, frame='circle'):\n",
    "    \"\"\"\n",
    "    Create a radar chart with `num_vars` axes.\n",
    "\n",
    "    This function creates a RadarAxes projection and registers it.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_vars : int\n",
    "        Number of variables for radar chart.\n",
    "    frame : {'circle', 'polygon'}\n",
    "        Shape of frame surrounding axes.\n",
    "\n",
    "    \"\"\"\n",
    "    # calculate evenly-spaced axis angles\n",
    "    theta = np.linspace(0, 2*np.pi, num_vars, endpoint=False)\n",
    "\n",
    "    class RadarAxes(PolarAxes):\n",
    "\n",
    "        name = 'radar'\n",
    "        # use 1 line segment to connect specified points\n",
    "        RESOLUTION = 1\n",
    "\n",
    "        def __init__(self, *args, **kwargs):\n",
    "            super().__init__(*args, **kwargs)\n",
    "            # rotate plot such that the first axis is at the top\n",
    "            self.set_theta_zero_location('N')\n",
    "\n",
    "        def fill(self, *args, closed=True, **kwargs):\n",
    "            \"\"\"Override fill so that line is closed by default\"\"\"\n",
    "            return super().fill(closed=closed, *args, **kwargs)\n",
    "\n",
    "        def plot(self, *args, **kwargs):\n",
    "            \"\"\"Override plot so that line is closed by default\"\"\"\n",
    "            lines = super().plot(*args, **kwargs)\n",
    "            for line in lines:\n",
    "                self._close_line(line)\n",
    "\n",
    "        def _close_line(self, line):\n",
    "            x, y = line.get_data()\n",
    "            # FIXME: markers at x[0], y[0] get doubled-up\n",
    "            if x[0] != x[-1]:\n",
    "                x = np.append(x, x[0])\n",
    "                y = np.append(y, y[0])\n",
    "                line.set_data(x, y)\n",
    "\n",
    "        def set_varlabels(self, labels):\n",
    "            self.set_thetagrids(np.degrees(theta), labels)\n",
    "\n",
    "        def _gen_axes_patch(self):\n",
    "            # The Axes patch must be centered at (0.5, 0.5) and of radius 0.5\n",
    "            # in axes coordinates.\n",
    "            if frame == 'circle':\n",
    "                return Circle((0.5, 0.5), 0.5)\n",
    "            elif frame == 'polygon':\n",
    "                return RegularPolygon((0.5, 0.5), num_vars,\n",
    "                                      radius=.5, edgecolor=\"k\")\n",
    "            else:\n",
    "                raise ValueError(\"Unknown value for 'frame': %s\" % frame)\n",
    "\n",
    "        def _gen_axes_spines(self):\n",
    "            if frame == 'circle':\n",
    "                return super()._gen_axes_spines()\n",
    "            elif frame == 'polygon':\n",
    "                # spine_type must be 'left'/'right'/'top'/'bottom'/'circle'.\n",
    "                spine = Spine(axes=self,\n",
    "                              spine_type='circle',\n",
    "                              path=Path.unit_regular_polygon(num_vars))\n",
    "                # unit_regular_polygon gives a polygon of radius 1 centered at\n",
    "                # (0, 0) but we want a polygon of radius 0.5 centered at (0.5,\n",
    "                # 0.5) in axes coordinates.\n",
    "                spine.set_transform(Affine2D().scale(.5).translate(.5, .5)\n",
    "                                    + self.transAxes)\n",
    "                return {'polar': spine}\n",
    "            else:\n",
    "                raise ValueError(\"Unknown value for 'frame': %s\" % frame)\n",
    "\n",
    "    register_projection(RadarAxes)\n",
    "    return theta\n",
    "\n",
    "def radar_plot(kmeans, df_scaled):\n",
    "    N  = kmeans.cluster_centers_.shape[1]  # number of columns / variables\n",
    "    k = kmeans.n_clusters\n",
    "    theta = radar_factory(N, frame='polygon')\n",
    "    data = kmeans.cluster_centers_.T\n",
    "    spoke_labels = [col for col in df_scaled.columns if col!='cluster_id']\n",
    "    fig, ax = plt.subplots(figsize=(9, 9),\n",
    "                                subplot_kw=dict(projection='radar'))\n",
    "    fig.subplots_adjust(wspace=0.25, hspace=0.20, top=0.85, bottom=0.05)\n",
    "\n",
    "    ax.plot(theta, data) #, color=color)\n",
    "    ax.set_varlabels(spoke_labels)\n",
    "\n",
    "    # add legend relative to top-left plot\n",
    "    labels = ['Cluster {}'.format(kk) for kk in range(k)]\n",
    "    ax.legend(labels, loc=(0.9, .95),\n",
    "                                labelspacing=0.1, fontsize='small')\n",
    "    \n",
    "# your code here\n",
    "radar_plot(kmeans, df_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8069be75",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Exercise:</strong> Experiment with different values of k (number of clusters), and using different columns.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f96e2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b1b7fd",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Exercise:</strong> The dataframe comes with x and y coordinates. Use them to identify any spatial clusters of cruising trips in San Francisco.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae74ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "cols = ['x','y']\n",
    "\n",
    "# let's not standardize, because we are working with latitude and longitude\n",
    "# so the variables are already on (roughly) the same scale\n",
    "#scaler = preprocessing.StandardScaler().fit(cruisingDf[cols])\n",
    "#df_scaled = pd.DataFrame(scaler.transform(cruisingDf[cols]), \n",
    "#                         columns=cols, index=cruisingDf.index)\n",
    "\n",
    "# restrict to San Francisco (otherwise we'll end up with a cluster in Ann Arbor)\n",
    "sfDf = cruisingDf[cruisingDf.sf==True][cols].dropna()\n",
    "\n",
    "# you can rerun this several times (through the map below) with different numbers of k\n",
    "kmeans = KMeans(n_clusters=20, random_state=1).fit(sfDf)\n",
    "\n",
    "sfDf['cluster_id'] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48526c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the long/lat of our cluster centers\n",
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f367f349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a geoDataFrame with just the geometry\n",
    "import geopandas as gpd\n",
    "centers = gpd.GeoDataFrame(geometry=\n",
    "            gpd.points_from_xy(kmeans.cluster_centers_[:,0], \n",
    "                               kmeans.cluster_centers_[:,1]),\n",
    "                          crs ='EPSG:4326')\n",
    "\n",
    "# we could have done this in two steps, which might be easier to read\n",
    "centers = pd.DataFrame(kmeans.cluster_centers_, columns=['lon','lat'])\n",
    "centers = gpd.GeoDataFrame(geometry=\n",
    "            gpd.points_from_xy(centers['lon'], \n",
    "                               centers['lat']),\n",
    "                          crs ='EPSG:4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9aade7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check it looks ok\n",
    "centers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab53a1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a geodataframe of the cruising points (so we can plot them as well)\n",
    "cruisingGdf = gpd.GeoDataFrame(sfDf, \n",
    "                geometry=gpd.points_from_xy(sfDf.x, sfDf.y,\n",
    "                    crs='EPSG:4326'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195ad230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import contextily as ctx\n",
    "fig, ax= plt.subplots(figsize=(10,10))\n",
    "\n",
    "# plot the cluster centers with a large marker\n",
    "centers.plot(ax=ax, markersize=100)\n",
    "\n",
    "# plot the individual points with a small marker, and a different color for each cluster_id\n",
    "cruisingGdf.plot(ax=ax, markersize=0.1, column='cluster_id')\n",
    "ctx.add_basemap(ax=ax, crs='EPSG:4326', alpha=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2818a26",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h3>What you should have learned</h3>\n",
    "<ul>\n",
    "  <li>Get more practice with spatial joins and Socrata.</li>\n",
    "  <li>Learn how to estimate a random forests model for continuous data.</li>\n",
    "  <li>Get more practice with standardizing data.</li>\n",
    "  <li>Learn how to estimate a k-means cluster analysis.</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4d2e00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
